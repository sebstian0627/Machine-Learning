{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kmeans.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1p8Yw4A17TxHjAV5ljAL504f9YTKz0qVS","authorship_tag":"ABX9TyO/xwOzQmmbaPdDtQ5UENxe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcBXEdLUuP7f","executionInfo":{"status":"ok","timestamp":1610989909993,"user_tz":-330,"elapsed":1087,"user":{"displayName":"ELL409 Assignment","photoUrl":"","userId":"11195655504482831460"}},"outputId":"51b16fa6-3441-435b-c1ae-554b20285add"},"source":["%cd /content/drive/MyDrive/Assignment_2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Assignment_2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t_N0vOBkv01i"},"source":["import scipy.io\r\n","import os\r\n","import matplotlib.pyplot as plt\r\n","import numpy as np\r\n","import cv2\r\n","from sklearn import svm\r\n","import heapq\r\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aE1sfNc1yxI2"},"source":["#loading svhn dataset\r\n","svhn=scipy.io.loadmat(\"train_32x32.mat\")\r\n","X=svhn[\"X\"]\r\n","y=svhn['y']\r\n","Data=np.zeros((73257,1024))\r\n","for i in range(73257):\r\n","  temp=cv2.cvtColor(X[:,:,:,i],cv2.COLOR_BGR2GRAY)\r\n","  temp=temp.flatten()\r\n","  Data[i,:]=temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gv3_g8SfyxCP"},"source":["def PCA(X , k):    \r\n","    X_shifted = X - np.mean(X , axis = 0)\r\n","    sigma = np.cov(X_shifted , rowvar = False)\r\n","    eigen_values , eigen_vectors = np.linalg.eigh(sigma)\r\n","    sorted_index = np.argsort(eigen_values)[::-1]\r\n","    sorted_eigenvalue = eigen_values[sorted_index]\r\n","    sorted_eigenvectors = eigen_vectors[:,sorted_index]\r\n","    eigenvector_subset = sorted_eigenvectors[:,0:k]\r\n","    X_reduced = np.dot(eigenvector_subset.transpose() , X_shifted.transpose() ).transpose()\r\n","    return X_reduced"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBzEKuNyw26-"},"source":["#standardizing data\r\n","mu=np.mean(Data)\r\n","sig=np.std(Data)\r\n","data=(Data-mu)/sig\r\n","k=10\r\n","data_p=PCA(data,20)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42zqDvc-zA5H"},"source":["class K_Means:\r\n","    def __init__(self, k=2, tol=0.00001, max_iter=3000):\r\n","        self.k = k\r\n","        self.tol = tol\r\n","        self.max_iter = max_iter\r\n","\r\n","    def fit(self,data):\r\n","\r\n","        self.centroids = {}\r\n","\r\n","        for i in range(self.k):\r\n","            self.centroids[i] = data[i]\r\n","\r\n","        for i in range(self.max_iter):\r\n","            self.classifications = {}\r\n","\r\n","            for i in range(self.k):\r\n","                self.classifications[i] = []\r\n","\r\n","            for featureset in data:\r\n","                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\r\n","                classification = distances.index(min(distances))\r\n","                self.classifications[classification].append(featureset)\r\n","\r\n","            prev_centroids = dict(self.centroids)\r\n","\r\n","            for classification in self.classifications:\r\n","                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\r\n","\r\n","            optimized = True\r\n","\r\n","            for c in self.centroids:\r\n","                original_centroid = prev_centroids[c]\r\n","                current_centroid = self.centroids[c]\r\n","                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\r\n","                    # print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\r\n","                    optimized = False\r\n","\r\n","            if optimized:\r\n","                break\r\n","\r\n","    def predict(self,data):\r\n","        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\r\n","        classification = distances.index(min(distances))\r\n","        return classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cbS502l2SZ22"},"source":["#prepares data for the Classifier, labels each cluster as a class.\r\n","def make_data(model):\r\n","  n=len(model.centroids)\r\n","  y=[]\r\n","  data=[]\r\n","  for i in model.centroids:\r\n","    data=data+model.classifications[i]\r\n","    y=y+[i]*len(model.classifications[i])\r\n","  return data,y\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE56fpFQVoPB"},"source":["#prepares label for test data\r\n","def make_y(test_x,model):\r\n","  y=np.zeros(test_x.shape[0])\r\n","  for i in range(test_x.shape[0]):\r\n","    y[i]=model.predict(test_x[i,:])\r\n","  return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSkLvJHfFiyG"},"source":["#Supervised learning part\r\n","data=data_p\r\n","np.random.shuffle(data)\r\n","data=data[:10000,:]\r\n","train_x=data[:80*10000//100,:]\r\n","test_x=data[80*10000//100:,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rXzV6TacgL2"},"source":["#evaluates SVM over the dataset\r\n","ks=[3,5,10,15,20]\r\n","results_linear=[]\r\n","for i in ks:\r\n","  model=K_Means(k=i,max_iter=10000000)\r\n","  model.fit(train_x)\r\n","  train_x,train_y=make_data(model)\r\n","  train_x=np.array(train_x)\r\n","  train_y=np.array(train_y)\r\n","  test_y=make_y(test_x,model)\r\n","  test_y=np.array(test_y)\r\n","  test_x=np.array(test_x)\r\n","\r\n","\r\n","  linear = svm.SVC(kernel='linear',  C=0.1, decision_function_shape='ovo').fit(train_x, train_y)\r\n","  accuracy_linear = linear.score(test_x, test_y)\r\n","  preds=linear.predict(test_x)\r\n","  cml=confusion_matrix(test_y,preds)\r\n","  results_linear.append((accuracy_linear,cml))\r\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAjIEoMEAyIX"},"source":["# Training a neural net for the same purpose.\r\n","from keras.models import Sequential\r\n","from keras.layers import Dense,Dropout\r\n","import keras\r\n","models_acc=[]\r\n","noc=3\r\n","for i in [3,5,10,15,20]:\r\n","  noc=i\r\n","  cluster_model=K_Means(k=i,max_iter=10000000)\r\n","  cluster_model.fit(train_x)\r\n","  train_x,train_y=make_data(cluster_model)\r\n","  train_x=np.array(train_x)\r\n","  train_y=np.array(train_y)\r\n","  test_y=make_y(test_x,cluster_model)\r\n","  test_y=np.array(test_y)\r\n","  test_x=np.array(test_x)\r\n","\r\n","  train_y=keras.utils.to_categorical(train_y,i)\r\n","  test_y=keras.utils.to_categorical(test_y,i)\r\n","  itt=5\r\n","  best_acc=-1\r\n","  for _ in range(itt):\r\n","    nn_model=Sequential()\r\n","    nn_model.add(Dense(32,input_shape=(20,),activation='relu'))\r\n","    nn_model.add(Dropout(0.2))\r\n","    nn_model.add(Dense(28, activation='relu'))\r\n","    nn_model.add(Dropout(0.2))\r\n","    nn_model.add(Dense(25, activation='relu'))\r\n","    nn_model.add(Dropout(0.2))\r\n","    nn_model.add(Dense(25, activation='relu'))\r\n","    nn_model.add(Dropout(0.2))\r\n","    nn_model.add(Dense(25, activation='relu'))\r\n","    nn_model.add(Dropout(0.2))\r\n","    nn_model.add(Dense(i, activation='sigmoid'))\r\n","\r\n","    nn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\r\n","  # nn_model.summary()\r\n","  \r\n","    nn_model.fit(train_x,train_y, epochs=96, batch_size=128)\r\n","    _, accuracy = nn_model.evaluate(test_x,test_y)\r\n","    if best_acc<accuracy:\r\n","      best_acc=accuracy\r\n","  models_acc.append(best_acc)\r\n","  print(\"hi\")"],"execution_count":null,"outputs":[]}]}
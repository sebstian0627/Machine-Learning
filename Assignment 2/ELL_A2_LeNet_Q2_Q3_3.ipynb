{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELL_A2_LeNet_Q2_Q3.3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLe_mmJ9kDwe"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "#import torch.utils.data as data\r\n",
        "\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as datasets\r\n",
        "\r\n",
        "from sklearn import decomposition\r\n",
        "from sklearn import manifold\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import math\r\n",
        "import copy\r\n",
        "import random\r\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukHh-C4ETWDJ"
      },
      "source": [
        "# from google.colab import drive\r\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqjbcb81UZ25"
      },
      "source": [
        "univseed = 42\r\n",
        "\r\n",
        "random.seed(univseed)\r\n",
        "np.random.seed(univseed)\r\n",
        "torch.manual_seed(univseed)\r\n",
        "torch.cuda.manual_seed(univseed)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSROuSFLca7X"
      },
      "source": [
        "# mu = [0.48093379,0.44808328,0.39650237]; sd = [0.22996924,0.22610814,0.22566715];\r\n",
        "# transf = transforms.Compose([transforms.Resize(256),transforms.CenterCrop(224),transforms.ToTensor(),transforms.Normalize(mu,sd)])\r\n",
        "# totdata = datasets.ImageFolder()\r\n",
        "\r\n",
        "# datapath = \"/content/drive/MyDrive/A2_data/\"\r\n",
        "# traindata = np.load(datapath + \"train_x.npy\", mmap_mode='r')\r\n",
        "# trainlabels = np.load(datapath + \"train_y.npy\", mmap_mode='r')\r\n",
        "# testdata = np.load(datapath + \"test_x.npy\", mmap_mode='r')\r\n",
        "# testlabels = np.load(datapath + \"test_y.npy\", mmap_mode='r')\r\n",
        "# # img1 = []\r\n",
        "# # img2 = []\r\n",
        "# # img3 = []\r\n",
        "# # for lab in traindata:\r\n",
        "# #   img1.append(lab[0])\r\n",
        "# #   img2.append(lab[1])\r\n",
        "# #   img3.append(lab[2])\r\n",
        "\r\n",
        "# # means = [np.mean(img1),np.mean(img2),np.mean(img3)]\r\n",
        "# # stdevs = [np.std(img1),np.std(img2),np.std(img3)]\r\n",
        "# # print(means)\r\n",
        "# # print(stdevs)\r\n",
        "\r\n",
        "# mean = np.mean(traindata) / 255\r\n",
        "# std = np.std(traindata) / 255\r\n",
        "# print(mean)\r\n",
        "# print(std)\r\n",
        "# transformations = transforms.Compose([\r\n",
        "#     transforms.Resize(255),\r\n",
        "#     transforms.CenterCrop(224),\r\n",
        "#     transforms.ToTensor(),\r\n",
        "#     transforms.Normalize(mean=[122.63811599026143, 114.26123747517829, 101.10810390651845], std=[70.52046150278801, 68.46892052533573, 71.82977751494695])])\r\n",
        "\r\n",
        "# trainlabels1 = []\r\n",
        "\r\n",
        "# for label in trainlabels:\r\n",
        "#   v = np.zeros(shape=(200,))\r\n",
        "#   #print(label)\r\n",
        "#   v[label] = 1\r\n",
        "#   trainlabels1.append(v)\r\n",
        "#   #print(v)\r\n",
        "\r\n",
        "# #  break\r\n",
        "# # trainlabels = np.asarray(trainlabels1)\r\n",
        "# # del trainlabels1\r\n",
        "# # print(trainlabels.shape)\r\n",
        "# testlabels1 = []\r\n",
        "# for label in testlabels:\r\n",
        "#   v = np.zeros(shape=(200,))\r\n",
        "#   v[label] = 1\r\n",
        "#   testlabels1.append(v)\r\n",
        "\r\n",
        "# # testabels = np.asarray(testlabels1)\r\n",
        "# # del testlabels1\r\n",
        "# # print(testabels.shape)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9um4koe4wbde"
      },
      "source": [
        "# transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor(),])\r\n",
        "transform = transforms.Compose([transforms.ToTensor()])\r\n",
        "mnist = datasets.MNIST('/tmp', download=True, transform=transform)\r\n",
        "mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\r\n",
        "                                            batch_size=21,\r\n",
        "                                            shuffle=True,\r\n",
        "                                            num_workers=2)\r\n",
        "\r\n",
        "mnist_test = datasets.MNIST('/tmp',train=False,transform=transform)\r\n",
        "mnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\r\n",
        "                                            batch_size=21,\r\n",
        "                                            shuffle=True,\r\n",
        "                                            num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCLse3tPpZHI"
      },
      "source": [
        "transform_sv = transforms.Compose([transforms.ToTensor(),transforms.Grayscale(1),transforms.Resize((28,28))]) #change normalize values if need be\r\n",
        "\r\n",
        "svhn = datasets.SVHN('/tmp', transform=transform_sv,download=True,split='train')\r\n",
        "svhn_test = datasets.SVHN('/tmp', split='test', transform=transform_sv,download=True)\r\n",
        "\r\n",
        "#mnist = torchvision.datasets.MNIST('/tmp', download=True, transform=transform)\r\n",
        "\r\n",
        "svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\r\n",
        "                                          batch_size=21,\r\n",
        "                                          shuffle=True,\r\n",
        "                                          num_workers=2)\r\n",
        "\r\n",
        "svhn_testload = torch.utils.data.DataLoader(dataset=svhn_test,\r\n",
        "                                            batch_size=21,\r\n",
        "                                            shuffle=True,\r\n",
        "                                            num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SuCi35PCjDP"
      },
      "source": [
        "# print(mnist_test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dltISIx2gQqw"
      },
      "source": [
        "# dataiter = iter(mnist_loader)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwEcqJhGaI1-"
      },
      "source": [
        "# validation_set = 0.1\r\n",
        "\r\n",
        "# n_train = int(len(traindata) * (1-validation_set))\r\n",
        "# n_valid = len(traindata) - n_train\r\n",
        "\r\n",
        "# validdata = traindata[:n_valid]\r\n",
        "# validlabels = trainlabels1[:n_valid]\r\n",
        "\r\n",
        "# traindata1 = traindata[n_valid:]\r\n",
        "# trainlabels1 = trainlabels1[n_valid:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC8E3mXyfrLm"
      },
      "source": [
        "class lenet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(lenet, self).__init__()\r\n",
        "        self.c1 = nn.Conv2d(1, 6, 5, padding=2)\r\n",
        "        self.c2 = nn.Conv2d(6, 16, 5)\r\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n",
        "        self.fc2 = nn.Linear(120, 84)\r\n",
        "        self.fc3 = nn.Linear(84, 10)\r\n",
        "                \r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.c1(x))\r\n",
        "        x = F.max_pool2d(x, 2)\r\n",
        "        x = F.relu(self.c2(x))\r\n",
        "        x = F.max_pool2d(x, 2)\r\n",
        "        x = x.view(x.size(0), -1)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "        \r\n",
        "        return x\r\n",
        "\r\n",
        "    \r\n",
        "model = lenet()\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print('Using device:', device)\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y053I4XhJZ_"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction='sum')\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99))\r\n",
        "#lrt = 0.001; momt = 0;\r\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lrt,momentum=momt)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROHnLHQfnuUP"
      },
      "source": [
        "epoch=10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNLDiUd42_ib"
      },
      "source": [
        "def findacc(testloader,net):\r\n",
        "  correct = 0\r\n",
        "  total = 0\r\n",
        "  with torch.no_grad():\r\n",
        "    for data in testloader:\r\n",
        " #     print(data.type())\r\n",
        "      images, labels = data\r\n",
        "      outputs = net(images)\r\n",
        "      _, predicted = torch.max(outputs.data, 1)\r\n",
        "      total += labels.size(0)\r\n",
        "      correct += (predicted == labels).sum().item()\r\n",
        "\r\n",
        "  return float(correct/total)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roZMXyZuoFLA"
      },
      "source": [
        "trainaccs = []\r\n",
        "testaccs = []\r\n",
        "losses = []\r\n",
        "nonzerowts = []\r\n",
        "identifier = None\r\n",
        "\r\n",
        "if(identifier==\"svhnTrain\"):\r\n",
        "  data_ld = svhn_loader\r\n",
        "else:\r\n",
        "  data_ld = mnist_loader\r\n",
        "\r\n",
        "for e in range(epoch):\r\n",
        "  tot_loss = 0.0\r\n",
        "  iv=0\r\n",
        "\r\n",
        "  for i, data in enumerate(data_ld, 0):\r\n",
        "    inputs, labels = data\r\n",
        "   # inputs, labels = inputs.cuda(), labels.cuda() # add this line\r\n",
        "\r\n",
        "    optimizer.zero_grad()\r\n",
        "    #print(inputs.shape)\r\n",
        "    inputs1 = inputs\r\n",
        "\r\n",
        "    if(identifier==\"3.3Algo2\"):\r\n",
        "      btch = inputs.shape[0]\r\n",
        "      subset = torch.utils.data.Subset(svhn, np.random.choice(len(svhn),btch))\r\n",
        "      testloader_subset = torch.utils.data.DataLoader(subset, batch_size=btch, num_workers=2, shuffle=False)\r\n",
        "      \r\n",
        "      for i,data in enumerate(testloader_subset,0):\r\n",
        "        vhnv = data[0]\r\n",
        "\r\n",
        "      inputs1 = torch.maximum(inputs,(np.random.randint(1,10,1)[0]/11)*vhnv) #change to 0.5*vhnv for other exp\r\n",
        "\r\n",
        "    outputs = model(inputs1)\r\n",
        "    loss = criterion(outputs, labels)\r\n",
        "    loss2 = loss\r\n",
        "\r\n",
        "    if(identifier==\"3.3Algo1\"):\r\n",
        "      btch = outputs.data.shape[0]\r\n",
        "      subset = torch.utils.data.Subset(svhn, np.random.choice(len(svhn),btch))\r\n",
        "      testloader_subset = torch.utils.data.DataLoader(subset, batch_size=btch, num_workers=2, shuffle=False)\r\n",
        "      for i,data in enumerate(testloader_subset,0):\r\n",
        "        loss1 = criterion(model(data[0]),labels)\r\n",
        " \r\n",
        "      loss2 = 0.95*loss + 0.05*loss1\r\n",
        "\r\n",
        "    loss2.backward()\r\n",
        "    optimizer.step()\r\n",
        "    tot_loss += loss2.item()\r\n",
        "    iv+=1\r\n",
        "\r\n",
        "  print('[%d, %5d] loss: %.3f' %(e + 1, i + 1, tot_loss /iv))\r\n",
        "  losses.append(tot_loss/iv)\r\n",
        "  \r\n",
        "  trainaccs.append(findacc(data_ld,model))\r\n",
        "\r\n",
        "  if identifier is None:\r\n",
        "    testaccs.append(findacc(mnist_loader,model))\r\n",
        "  elif(identifier==\"svhnTrain\"):\r\n",
        "    testaccs.append(findacc(mnist_loader,model))\r\n",
        "    \r\n",
        "  else:\r\n",
        "    testaccs.append(findacc(svhn_loader,model))\r\n",
        "  \r\n",
        "  zeros = 0\r\n",
        "  allz = 0\r\n",
        "  for param in model.parameters():\r\n",
        "    if param is not None:\r\n",
        "      zeros += param.nonzero().size(0)\r\n",
        "      allz+=param.numel()\r\n",
        "  nonzerowts.append(float(zeros/allz))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRI_JxMhBh2G"
      },
      "source": [
        "# print(trainaccs)\r\n",
        "# print(losses)\r\n",
        "# print(testaccs)\r\n",
        "# print(nonzerowts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOa4GVU9FQMe"
      },
      "source": [
        "# ------plots------------\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# plt.plot(range(10),tr1[:10],label=\"SVHN 0.5\")\r\n",
        "# plt.plot(range(len(tr2)),tr2,label=\"SVHN no use\")\r\n",
        "# plt.plot(range(len(tr3)),tr3,label=\"SVHN random wt\")\r\n",
        "\r\n",
        "# plt.title(\"LeNet, cross entropy, Adam\")\r\n",
        "# plt.ylabel(\"Accuracies\")\r\n",
        "# plt.xlabel(\"Epochs\")\r\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIV1kQmaofYK"
      },
      "source": [
        "\r\n",
        "# plt.plot(range(epoch),trainaccs,label=\"MNIST train accuracy\")\r\n",
        "# plt.plot(range(epoch),testaccs,label=\"SVHN train accuracy\")\r\n",
        "# plt.title(\"LeNet, cross entropy, Adam\")\r\n",
        "# plt.ylabel(\"Accuracies\")\r\n",
        "# plt.xlabel(\"Epochs\")\r\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFNIoDDR4M52"
      },
      "source": [
        "# plt.plot(range(epoch),losses,label=\"train accuracy\")\r\n",
        "# # plt.plot(range(epoch),testaccs,label=\"test accuracy\")\r\n",
        "# plt.title(\"LeNet, cross entropy, Adam\")\r\n",
        "# plt.ylabel(\"Loss\")\r\n",
        "# plt.xlabel(\"Epochs\")\r\n",
        "# # plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q3 ya nahi pata nahi shayad yahi hai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdVLcZqcQXE8"
      },
      "source": [
        "#IMPORTS\n",
        "import random\n",
        "import csv\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "from glob import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdzdxGOqQdME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "17cce21f-3c73-4b0d-82e7-fa618a8294ff"
      },
      "source": [
        "# Extract 3rd Problem Image zip\n",
        "os.mkdir(\"Dataset\")\n",
        "file_name = \"Medical_MNIST.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    os.chdir(\"Dataset\")\n",
        "    zip.extractall()\n",
        "os.chdir(\"../\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufnG0eswQd6Q",
        "outputId": "5141d91b-d983-45ef-dc1a-c863390774ee"
      },
      "source": [
        "# Reads Images for 3rd Problem\n",
        "print(os.getcwd())\n",
        "pp=[x[0] for x in os.walk(\"Dataset\")]\n",
        "data=[]\n",
        "print(list(pp))\n",
        "os.chdir(\"Dataset\")\n",
        "for i in range(1,len(pp)):\n",
        "  os.chdir(\"../\")\n",
        "  os.chdir(pp[i])\n",
        "  image_list=glob(\"*.jpeg\")\n",
        "  for mi in image_list:\n",
        "    image=cv2.imread(mi,cv2.IMREAD_GRAYSCALE)\n",
        "    image = cv2.resize(image,(32,32),interpolation = cv2.INTER_AREA)\n",
        "    d= image.flatten()\n",
        "    data.append(np.append(d,i)) \n",
        "  os.chdir(\"../\")\n",
        "os.chdir(\"../\")\n",
        "print(len(data))\n",
        "data1 =data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for random sampellling of data\n",
        "data = np.array(data)\n",
        "data = np.random.permutation(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# accounting for all possible outcomes\n",
        "possible_outputs = [1,2,3,4,5,6]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qwm8cfFlkKY"
      },
      "source": [
        "# Runs PCA to reduce Dimensionality of the Data\n",
        "def PCA(data_X,k):\n",
        "  X_mean=np.mean(X,axis=0)\n",
        "  X_cent=data_X-X_mean\n",
        "  cov_matrix=np.cov(X_cent,rowvar=False)\n",
        "  eigen_values,eigen_vec=np.linalg.eigh(cov_matrix)\n",
        "  index=np.argsort(eigen_values)\n",
        "  index=np.flip(index)\n",
        "  best_eigen=eigen_values[index]\n",
        "  best_eigen_vec=eigen_vec[:,index]\n",
        "  selected_eigen_vec=best_eigen_vec[:,:k]\n",
        "  data_X_reduced=np.dot(selected_eigen_vec.transpose(),X_cent.transpose()).transpose()\n",
        "  return data_X_reduced,best_eigen\n",
        "X = (data.T[:-1]).T\n",
        "y = (data.T[-1]).T\n",
        "X,ev = PCA(X,5)\n",
        "print(ev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normalizes Data\n",
        "X_normed = X / np.abs(X).max(axis=0)\n",
        "X = X_normed*100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66fDB8UCQhOl"
      },
      "source": [
        "#Divides Data into train and test sets and other processing into dictionaries and sets\n",
        "n = len(X)\n",
        "data_tr = X[:(80*n)//100]\n",
        "data_tt = X[(80*n)//100:]\n",
        "label_tr = y[:(80*n)//100]\n",
        "label_tt = y[(80*n)//100:]\n",
        "label_tt = np.reshape(label_tt,(label_tt.shape[0],1))\n",
        "label_tr = np.reshape(label_tr,(label_tr.shape[0],1))\n",
        "data_ttt = np.append(data_tt.T,label_tt.T,axis=0).T\n",
        "data_trr = np.append(data_tr.T,label_tr.T,axis=0).T\n",
        "wo_data_tr_hai = dict()\n",
        "wo_data_tt_hai = dict()\n",
        "for i in possible_outputs:\n",
        "    wo_data_tr_hai[i] = []\n",
        "    wo_data_tt_hai[i] = []\n",
        "for i in range(len(label_tt)):\n",
        "    wo_data_tt_hai[label_tt[i][0]].append(data_tt[i])\n",
        "for i in range(len(label_tr)):\n",
        "    wo_data_tr_hai[label_tr[i][0]].append(data_tr[i])\n",
        "dicto = dict()\n",
        "for i in wo_data_tr_hai.items():\n",
        "    dicto[i[0]] = len(i[1])/label_tr.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ML_vKGVnvF"
      },
      "source": [
        "#KNN code Block \n",
        "def distance(a,b):\n",
        "  return (int(a)-int(b))*(int(a)-int(b))\n",
        "\n",
        "# predicts label for a point\n",
        "def knn(data,point,k): \n",
        "  n = data.shape[0]\n",
        "  o = len(point)\n",
        "  heap = []\n",
        "  for i in range(n):\n",
        "    dist = 0\n",
        "    for j in range(o):\n",
        "      dist += distance(data[i][j],point[j])\n",
        "    heap.append((dist,data[i][o]))\n",
        "  heapq.heapify(heap)\n",
        "  freq = dict()\n",
        "  for i in range(k):\n",
        "    temp = heapq.heappop(heap)[1]\n",
        "    freq[temp]=freq.get(temp,0)+1\n",
        "  max = 0\n",
        "  result = 0\n",
        "  for i in freq.items():\n",
        "    if max<i[1]:\n",
        "      max = i[1]\n",
        "      result = i[0]\n",
        "  return int(result)\n",
        "\n",
        "# calls predict on test set and outputs various result parameters Confusion Matrix,Per class Accuraccy prescion F1 Score and Macro F1 Score\n",
        "def knn_caller(data,test,k):\n",
        "  n = test.shape[0]\n",
        "  o = data.shape[1]\n",
        "  macroF1 = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in range(n):\n",
        "    lis = []\n",
        "    for j in range(o-1):\n",
        "      lis.append(test[i][j])\n",
        "    temp = knn(data,lis,k)\n",
        "    dp[temp-1][int(test[i][o-1]-1)]+=1\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate Caller for KNN used for getting Confusion matrix over trining data ( nothing model related just result of bad coding practice :P )\n",
        "def knn_train(wo_data_tr_hai,wo_data_tt_hai):\n",
        "    lis = []\n",
        "    y = []\n",
        "    lis2 = []\n",
        "    y2 = []\n",
        "    global weights\n",
        "    for i in wo_data_tr_hai.items():\n",
        "        for j in i[1]:\n",
        "            lis.append(j)\n",
        "            y.append(i[0])\n",
        "    for i in wo_data_tt_hai.items():\n",
        "        for j in i[1]:\n",
        "            lis2.append(j)\n",
        "            y2.append(i[0])\n",
        "    lis = np.array(lis)\n",
        "    lis2 = np.array(lis2)\n",
        "    y = np.reshape(np.array(y),(1,len(y)))\n",
        "    y2 = np.reshape(np.array(y2),(1,len(y2)))\n",
        "    return knn_caller(np.append(lis.T,y,axis=0).T,np.append(lis2.T,y2,axis=0).T,2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWifjcP4n56L"
      },
      "source": [
        "# for running bias - variance over KNN calling knn for multiple K's \n",
        "results = []\n",
        "for i in range(1,10):\n",
        "    results.append(knn_caller( np.append(data_tt,label_tt,axis = 1),np.append(data_tr,label_tr,axis = 1),i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for plotting results \n",
        "graph = []\n",
        "for i in range(9):\n",
        "    graph.append(results[i][2])\n",
        "plt.plot(np.arange(1,50),np.array(graph))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMCQCXprYqUw"
      },
      "source": [
        "#PARZEN WINDOW code\n",
        "\n",
        "# checks if point2 lies in window around point\n",
        "def valid(point,point2,parameter):\n",
        "  n = len(parameter)\n",
        "  for i in range(n):\n",
        "    if abs(int(point2[i]) - int(point[i]))/parameter[i] > 0.5:\n",
        "      return 0\n",
        "  return 1\n",
        "\n",
        "# Returns label for point given train data(Data) and window size (Parameter)\n",
        "def parzen(data,point,parameter): \n",
        "  n = len(data)\n",
        "  o = len(parameter)\n",
        "  freq = dict()\n",
        "  for i in range(n):\n",
        "    freq[data[i][o]] = freq.get(data[i][o],0) + valid(point,data[i],parameter)\n",
        "  max = 0\n",
        "  result = 0\n",
        "  for i in freq.items():\n",
        "    if max<i[1]:\n",
        "      max = i[1]\n",
        "      result = i[0]\n",
        "  return result\n",
        "\n",
        "#Evaluates over entire test set and returns the result parameters\n",
        "def parzen_caller(data,test,parameter):\n",
        "  n = len(test)\n",
        "  o = len(data[0])\n",
        "  right = 0\n",
        "  macroF1 = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in range(n):\n",
        "    temp = parzen(data,test[i],parameter)\n",
        "    dp[int(temp-1)][int(test[i][o-1]-1)]+=1\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternate Caller for Parzen used for getting Confusion matrix over trining data ( nothing model related just result of bad coding practice yet again :P )\n",
        "def parzen_train(wo_data_tr_hai,wo_data_tt_hai):\n",
        "    lis = []\n",
        "    y = []\n",
        "    lis2 = []\n",
        "    y2 = []\n",
        "    global weights\n",
        "    for i in wo_data_tr_hai.items():\n",
        "        for j in i[1]:\n",
        "            lis.append(j)\n",
        "            y.append(i[0])\n",
        "    for i in wo_data_tt_hai.items():\n",
        "        for j in i[1]:\n",
        "            lis2.append(j)\n",
        "            y2.append(i[0])\n",
        "    lis = np.array(lis)\n",
        "    lis2 = np.array(lis2)\n",
        "    y = np.reshape(np.array(y),(1,len(y)))\n",
        "    y2 = np.reshape(np.array(y2),(1,len(y2)))\n",
        "    return parzen_caller(np.append(lis.T,y,axis=0).T,np.append(lis2.T,y2,axis=0).T,[25]*5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "  return 1/(1+np.exp(-1*X))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yODFA5qpAAT",
        "tags": []
      },
      "source": [
        "# logestic regression with all regularizer cross entropy\n",
        "\n",
        "# some data preproceesing (adding 1 toa acount for w0)\n",
        "def data_magic(X):\n",
        "  nupea = []\n",
        "  for i in range(len(X)):\n",
        "    nupea.append( np.append(X[i],[1]) )\n",
        "  return np.array(nupea)\n",
        "\n",
        "# trains a logetistic regression for one label (one vs all used)\n",
        "def loget_elastic_net(X,y,alpha,max_iter,lambda1,lambda2):\n",
        "  X = data_magic(X)\n",
        "  m=X.shape[0]\n",
        "  n=X.shape[1]\n",
        "  w = np.zeros(X.shape[1])\n",
        "  y = np.reshape(y,(len(y),1))\n",
        "  for i in range(max_iter):\n",
        "    sgd = np.random.randint(m,size=(64))\n",
        "    Xt = np.take(X,sgd,axis=0)\n",
        "    yt = np.take(y,sgd)\n",
        "    grad=Xt.T@(sigmoid(Xt@w)-yt)/64\n",
        "    for j in range(n-1):\n",
        "       if w[j]>0:\n",
        "         grad[j]+=(lambda1+2*lambda2*w[j])/m\n",
        "       else:\n",
        "         grad[j]+=(-lambda1+2*lambda2*w[j])/m\n",
        "    w=w-alpha*grad \n",
        "  return w\n",
        "\n",
        "# trains a logetistic regression for all labels\n",
        "def model_loget(X,y,alpha,max_iter,lambda1,lambda2,k=6):\n",
        "  k = 6\n",
        "  y1=np.zeros(len(y))\n",
        "  weights=[]\n",
        "  for i in range(1,7):\n",
        "    y1=np.where(y==i,1,0)\n",
        "    a =loget_elastic_net(X,y1,alpha,max_iter,lambda1,lambda2)\n",
        "    weights.append(a)\n",
        "  return weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predicts label for a new input\n",
        "def predict_loget(x):\n",
        "  probab = []\n",
        "  for i in weights:\n",
        "    probab.append(sigmoid((np.append(x,1))@i))\n",
        "  return (np.argsort(np.array(probab))[::-1])[0] + 1\n",
        "    \n",
        "#evaluates entire train set and returns relevant results for logisitic regression\n",
        "def eval1(wo_data_tt_hai):\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in wo_data_tt_hai.items():\n",
        "    for j in i[1]:\n",
        "      pre = int(predict_loget(j))\n",
        "      dp[pre-1][i[0]-1]+=1\n",
        "  macroF1 = 0\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# alternate loget caller for convience \n",
        "def loget_train(wo_data_tr_hai):\n",
        "    lis = []\n",
        "    y = []\n",
        "    global weights\n",
        "    for i in wo_data_tr_hai.items():\n",
        "        for j in i[1]:\n",
        "            lis.append(j)\n",
        "            y.append(i[0])\n",
        "    print(y)\n",
        "    weights = model_loget(lis,np.array(y),0.005,10000,2,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# given confusion matrix evaluates per class acuuracy presion recall etc\n",
        "def confusion(dp,k):\n",
        "  tp = 0 \n",
        "  tn = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  for i in range(6):\n",
        "      for j in range(6):\n",
        "          if i==j==k:\n",
        "              tp+=dp[i][j]\n",
        "          elif i==k and j!=k:\n",
        "              fp+=dp[i][j]\n",
        "          elif j==k and i!=k:\n",
        "              fn+=dp[i][j]\n",
        "          else:\n",
        "              tn+=dp[i][j]\n",
        "  prescion = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  f1 = 2/(1/recall+1/prescion)\n",
        "  accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "  return accuracy,prescion,f1,recall,tp,fp,tn,fn\n",
        "def outputs(dp):\n",
        "    properties = []\n",
        "    for i in range(6):\n",
        "        properties.append(confusion(dp,i))\n",
        "    return np.array(properties)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDlW12PlpgzQ"
      },
      "source": [
        "#fits gaussian model for a given data\n",
        "def fit_gaussian(data_tr):\n",
        "  data_tr = np.array(data_tr)\n",
        "  return (np.reshape(np.mean(data_tr,axis=0),(20,1)),np.cov(data_tr.T,rowvar=1))\n",
        "\n",
        "# trains gaussian for all labels\n",
        "dict1 = {}\n",
        "def train(wo_data_tr_hai):\n",
        "  global dict1 , dicto\n",
        "  dict1 = {}\n",
        "  for i in wo_data_tr_hai.items():\n",
        "    dicto[i[0]] = len(i[1])/label_tr.shape[0]\n",
        "  for i in wo_data_tr_hai.items():\n",
        "    dict1[i[0]] = fit_gaussian(i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "e = math.exp(1)\n",
        "# multivariate logarithm of a gaussian function\n",
        "def gaussian(x,sigma,u,n):\n",
        "  x = np.reshape(x,(x.shape[0],1))\n",
        "  const =  1/math.sqrt( ( abs(np.linalg.det(sigma)) ) ) \n",
        "  other_term = ((np.transpose(x-u))@(np.linalg.pinv(sigma)))@(x-u)\n",
        "  return math.log(const,e)  + -0.5*other_term\n",
        "\n",
        "# predicts label for a input\n",
        "def predict(x):\n",
        "  max = -1*math.inf\n",
        "  label = \"default label\"\n",
        "  for i in dict1.items():\n",
        "    temp = gaussian(x,i[1][1],i[1][0],len(x))\n",
        "    if max < temp + math.log(dicto[i[0]],e):\n",
        "      label = i[0]\n",
        "      max = temp + math.log(dicto[i[0]],e)\n",
        "  return label\n",
        "  \n",
        "# evaluates test data and returns relevant information\n",
        "def check(wo_data_tt_hai):\n",
        "  right = 0\n",
        "  l = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in wo_data_tt_hai.items():\n",
        "    for j in i[1]:\n",
        "      pre = int(predict(j))\n",
        "      dp[pre-1][i[0]-1]+=1\n",
        "      if i[0] == pre:\n",
        "        right+=1\n",
        "  macroF1 = 0\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#splits the train data into size of 5 for 5 cross analysis\n",
        "lis = []\n",
        "for j in wo_data_tr_hai.items():\n",
        "    lis2 = []\n",
        "    ss = np.array(j[1])\n",
        "    n = ss.shape[0]\n",
        "    for i in range(5):\n",
        "        lis2.append(ss[(i*n)//5:((i+1)*n)//5])\n",
        "    lis.append(lis2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# runs 5 corss validation over train data and returns Confusion matrix\n",
        "Confusion_in_this_question = np.zeros((6,6))\n",
        "for i in range(5):\n",
        "    test = dict()\n",
        "    train1 = dict()\n",
        "    for j in range(5):\n",
        "        for k in range(6):\n",
        "            if j!=i:\n",
        "                if k+1 in train1.keys():\n",
        "                    train1[k+1] = np.append(train1[k+1],lis[k][j],axis=0)\n",
        "                else:\n",
        "                    train1[k+1] = lis[k][j]\n",
        "            else:\n",
        "                test[k+1] = lis[k][i]\n",
        "    #training function\n",
        "    #test function\n",
        "    Confusion_in_this_question += knn_train(train1,test)[0]/5\n",
        "print(Confusion_in_this_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fits unimodal gaussian over entire data\n",
        "def sasta_fit_gaussian(data_tr):\n",
        "  data_tr = np.array(data_tr)\n",
        "  return (np.mean(data_tr),np.var(data_tr))\n",
        "dict1 = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Just due to bad naming practices :(\n",
        "possible_outcomes = possible_outputs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# univariate gaussian is sasta gaussian sorry :\\\n",
        "# Naive Bayes Caller logic \n",
        "e = math.exp(1)\n",
        "#univariate gaussian evaluate \n",
        "def sasta_gaussian(x,sigma,u):\n",
        "  return (1/math.sqrt(math.pi*2*sigma))*math.exp(-0.5*((x-u)*(x-u)/sigma)) \n",
        "\n",
        "# evaluates models for all naive bayes\n",
        "def naive_bayes(data_tr):\n",
        "  n = 20\n",
        "  models = np.zeros((20,6,2))\n",
        "  for i in range(n):\n",
        "    for j in possible_outcomes:\n",
        "      models[i][j-1][0],models[i][j-1][1] = sasta_fit_gaussian( (np.array(data_tr[j]).T[i]) ) \n",
        "  return models\n",
        "\n",
        "models = naive_bayes(wo_data_tr_hai)\n",
        "\n",
        "# Evaluates test data and returns releveant results \n",
        "def eval(data_tt):\n",
        "  l = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in wo_data_tt_hai.items():\n",
        "    for j in i[1]:\n",
        "      pre = int(predict_n(j))\n",
        "      dp[pre-1][i[0]-1]+=1\n",
        "  macroF1 = 0\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1\n",
        "\n",
        "# predicts labal for a given point\n",
        "def predict_n(x):\n",
        "  max = -1*math.inf\n",
        "  label = 'default label'\n",
        "  for i in range(len(possible_outcomes)):\n",
        "    temp = dicto[i+1]\n",
        "    for j in range(20):\n",
        "      temp*=sasta_gaussian(x[j],models[j][i][1],models[j][i][0])\n",
        "    if temp > max:\n",
        "      max = temp\n",
        "      label = possible_outcomes[i]\n",
        "  return label \n",
        "\n",
        "print(eval(wo_data_tt_hai))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EM algoritm for GMM returns mu ,wights and sigma\n",
        "e =math.exp(1)\n",
        "def GM(data,iteration,k):\n",
        "  data = np.array(data)\n",
        "  dimension = len(data[0])\n",
        "  n = len(data)\n",
        "  pk = np.array( [1/k]*k )\n",
        "  mu = np.array( np.random.uniform(low=0.01,high=0.02, size=(k,dimension,1)) )\n",
        "  sigma = np.array( np.random.uniform(low=0.001,high=1000, size=(k,dimension,dimension)) )\n",
        "  for iter in range(iteration):\n",
        "    tmu = np.zeros((k,dimension,1))\n",
        "    tsigma = np.zeros((k,dimension,dimension))\n",
        "    tpk = np.zeros((k,1))\n",
        "    global_sum = 0 \n",
        "\n",
        "    dp = np.zeros((n,k))\n",
        "    #E - Step\n",
        "    for i in range(n):\n",
        "      normal = 0\n",
        "      for j in range(k):\n",
        "        oo = np.reshape(data[i],(data[i].shape[0],1))\n",
        "        dp[i][j] = pk[j]*math.exp(gaussian(oo,sigma[j],mu[j],dimension))\n",
        "        normal += dp[i][j]\n",
        "      dp[i]/=normal\n",
        "      for j in range(k):\n",
        "        global_sum+=dp[i][j]\n",
        "        tpk[j]+=dp[i][j]\n",
        "      \n",
        "    # M - step\n",
        "    for i in range(n):\n",
        "      for j in range(k):\n",
        "        oo = np.reshape(data[i],(data[i].shape[0],1))\n",
        "        factor =  (dp[i][j]/tpk[j])\n",
        "        tmu[j] += factor*oo\n",
        "        oj = (oo-mu[j])@(np.transpose(oo-mu[j]))\n",
        "        tsigma[j] += factor*oj\n",
        "\n",
        "    for i in range(k):\n",
        "      tpk[i]/=global_sum\n",
        "\n",
        "    #Exchange \n",
        "    pk = tpk.copy()\n",
        "    sigma = tsigma.copy()\n",
        "    mu = tmu.copy()\n",
        "  return [pk,mu,sigma]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# caller to train GMM\n",
        "pp = dict()\n",
        "def gmmtrain(wo_data_tr_hai,k):\n",
        "    global pp\n",
        "    pp = dict()\n",
        "    for i in wo_data_tr_hai.items():\n",
        "        pp[i[0]] = GM(i[1],50,k)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#returns label for a point x \n",
        "def predict_gmm(x,k):\n",
        "  probab = []\n",
        "  for i in pp.items():\n",
        "    sum = 0\n",
        "    for j in range(k):\n",
        "      sum += i[1][0][j]*math.exp(gaussian(x,i[1][2][j],i[1][1][j],len(x)))\n",
        "    probab.append(dicto[i[0]]*sum)\n",
        "  probab = np.array(probab)\n",
        "  max = 0\n",
        "  label = -1\n",
        "  for i in range(6):\n",
        "      if probab[i] > max:\n",
        "          max = probab[i]\n",
        "          label = i+1 \n",
        "  return label\n",
        "\n",
        "# Evaluates test set and returns results like confusion matrix macro F1 etc\n",
        "def check_gmm(wo_data_tt_hai,k):\n",
        "  right = 0\n",
        "  l = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in wo_data_tt_hai.items():\n",
        "    for j in i[1]:\n",
        "      pre = int(predict_gmm(j,k))\n",
        "      dp[pre-1][i[0]-1]+=1\n",
        "      if i[0] == pre:\n",
        "        right+=1\n",
        "  macroF1 = 0\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#return train error for GMM model\n",
        "def train_error_gmm(k):\n",
        "  right = 0\n",
        "  l = 0\n",
        "  dp = np.zeros((6,6))\n",
        "  for i in wo_data_tr_hai.items():\n",
        "    for j in i[1]:\n",
        "      pre = int(predict_gmm(j,k))\n",
        "      dp[pre-1][i[0]-1]+=1\n",
        "      if i[0] == pre:\n",
        "        right+=1\n",
        "  macroF1 = 0\n",
        "  properties = outputs(dp)\n",
        "  for i in range(6):\n",
        "    macroF1 += properties[i][2]/6\n",
        "  return dp,properties,macroF1"
      ]
    }
  ]
}